{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Best Model Training and Sample Generation"
      ],
      "metadata": {
        "id": "07-0wskBFu2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "CcK0DYEMFu2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kZl0qZwjFu20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ceed46f-964f-4db6-cb01-e0f6ad651016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = Path('/content/drive/MyDrive/MLProject/data')\n",
        "output_dir = Path('/content/drive/MyDrive/MLProject/results')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "AKUiFbs9Fu20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "rc9NByxVFu20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af159c0-3a8c-4453-8733-9285acf5617e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "Memory: 85.2 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "r71FVZ3WFu20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load(data_dir / 'train.npy')\n",
        "val_data = np.load(data_dir / 'val.npy')\n",
        "test_data = np.load(data_dir / 'test.npy')\n",
        "\n",
        "with open(data_dir / 'tokenizer.json', 'r') as f:\n",
        "    token2idx = json.load(f)\n",
        "\n",
        "idx2token = {v: k for k, v in token2idx.items()}\n",
        "vocab_size = len(token2idx)\n",
        "\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "print(f\"Train tokens: {len(train_data):,}\")\n",
        "print(f\"Val tokens: {len(val_data):,}\")\n",
        "print(f\"Test tokens: {len(test_data):,}\")"
      ],
      "metadata": {
        "id": "bC2RH98BFu21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6687fe83-13b5-4463-fbbd-76e7ad68735a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 27224\n",
            "Train tokens: 1,167,894,118\n",
            "Val tokens: 11,907,471\n",
            "Test tokens: 11,808,149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Part 4 Indices"
      ],
      "metadata": {
        "id": "sogB-IpKFu21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_LENGTH = 256\n",
        "NEW_NUM_SAMPLES = 100_000_000 // CONTEXT_LENGTH\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "train_max_idx = len(train_data) - CONTEXT_LENGTH - 1\n",
        "val_max_idx = len(val_data) - CONTEXT_LENGTH - 1\n",
        "\n",
        "train_indices_part4 = np.random.choice(train_max_idx, size=NEW_NUM_SAMPLES, replace=False)\n",
        "val_indices = np.load(data_dir / 'val_indices.npy')\n",
        "\n",
        "np.save(data_dir / 'train_indices_part4.npy', train_indices_part4)\n",
        "\n",
        "print(f\"Part 4 train indices: {len(train_indices_part4):,}\")\n",
        "print(f\"Val indices: {len(val_indices):,}\")"
      ],
      "metadata": {
        "id": "-9oIi8zBFu21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b107e78-c22e-4f55-9aef-d3a2d5f8ece7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 4 train indices: 390,625\n",
            "Val indices: 39,062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "krbIMYsLFu21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset(Dataset):\n",
        "    def __init__(self, data, context_length, indices):\n",
        "        self.data = torch.from_numpy(data.astype(np.int64))\n",
        "        self.context_length = context_length\n",
        "        self.indices = indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = self.indices[idx]\n",
        "        x = self.data[start:start + self.context_length]\n",
        "        y = self.data[start + 1:start + self.context_length + 1]\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "QqLrCR5GFu21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Model"
      ],
      "metadata": {
        "id": "Vz9_gPp7Fu21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return self.out_proj(out)"
      ],
      "metadata": {
        "id": "p1C7kL6JFu22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(F.gelu(self.fc1(x))))"
      ],
      "metadata": {
        "id": "TQOBWiFaFu22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.dropout(self.attn(self.ln1(x), mask))\n",
        "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "X6uMPfhdFu22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, context_length, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.context_length = context_length\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(context_length, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        d_ff = 4 * d_model\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        self.register_buffer('mask', torch.tril(torch.ones(context_length, context_length)).unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
        "        x = self.dropout(self.token_emb(x) + self.pos_emb(positions))\n",
        "\n",
        "        mask = self.mask[:, :, :seq_len, :seq_len]\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "lBOLx8ShFu22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Configuration"
      ],
      "metadata": {
        "id": "TJb-zWQXFu22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_LENGTH = 256\n",
        "BATCH_TOKENS = 4096\n",
        "BATCH_SIZE = BATCH_TOKENS // CONTEXT_LENGTH\n",
        "LEARNING_RATE = 3e-4\n",
        "WARMUP_RATIO = 0.05\n",
        "\n",
        "MODEL_CONFIG = {'n_layers': 8, 'd_model': 1024, 'n_heads': 16}\n",
        "\n",
        "print(f\"Context length: {CONTEXT_LENGTH}\")\n",
        "print(f\"Batch size: {BATCH_SIZE} sequences ({BATCH_TOKENS} tokens)\")\n",
        "print(f\"Samples: {len(train_indices_part4):,}\")\n",
        "print(f\"Steps: {len(train_indices_part4) // BATCH_SIZE:,}\")"
      ],
      "metadata": {
        "id": "lKtKmamAFu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55818f10-f799-4984-dfdf-29af8ce6dc40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context length: 256\n",
            "Batch size: 16 sequences (4096 tokens)\n",
            "Samples: 390,625\n",
            "Steps: 24,414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pre-trained Model"
      ],
      "metadata": {
        "id": "BLJwHMgyFu22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=MODEL_CONFIG['d_model'],\n",
        "    n_heads=MODEL_CONFIG['n_heads'],\n",
        "    n_layers=MODEL_CONFIG['n_layers'],\n",
        "    context_length=CONTEXT_LENGTH\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(output_dir / 'xl_model.pt'))\n",
        "print(f\"Model loaded\")\n",
        "print(f\"Parameters: {model.count_parameters():,}\")"
      ],
      "metadata": {
        "id": "pdOZ6BcUFu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e93c747-d590-41b1-dc1a-28b7c6b7b540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded\n",
            "Parameters: 156,788,736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resume Training"
      ],
      "metadata": {
        "id": "1zAdrc6iFu22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(step, total_steps, warmup_steps, max_lr):\n",
        "    if step < warmup_steps:\n",
        "        return max_lr * step / warmup_steps\n",
        "    else:\n",
        "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
        "        return max_lr * 0.5 * (1 + math.cos(math.pi * progress))"
      ],
      "metadata": {
        "id": "1AmzsS20Fu22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset(train_data, CONTEXT_LENGTH, train_indices_part4)\n",
        "val_dataset = MusicDataset(val_data, CONTEXT_LENGTH, val_indices)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.1)\n",
        "\n",
        "total_steps = len(train_loader)\n",
        "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "\n",
        "print(f\"Total steps: {total_steps:,}\")\n",
        "print(f\"Warmup steps: {warmup_steps:,}\")"
      ],
      "metadata": {
        "id": "XmIk89P9Fu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a32d84a-b4b4-4bfa-cc45-747a5e8d96d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total steps: 24,415\n",
            "Warmup steps: 1,220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model.train()\n",
        "for step, (x, y) in enumerate(train_loader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    lr = get_lr(step, total_steps, warmup_steps, LEARNING_RATE)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    if step % 500 == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        remaining = (elapsed / (step + 1)) * (total_steps - step - 1)\n",
        "        print(f\"Step {step}/{total_steps} | Loss: {loss.item():.4f} | LR: {lr:.6f} | Time: {elapsed/60:.1f}m | ETA: {remaining/60:.1f}m\")\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {train_time/60:.1f} minutes\")"
      ],
      "metadata": {
        "id": "gdk7P5E8Fu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b85890cf-f88a-4663-fc54-3f1dd5a76338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0/24415 | Loss: 0.2398 | LR: 0.000000 | Time: 0.0m | ETA: 559.2m\n",
            "Step 500/24415 | Loss: 0.5309 | LR: 0.000123 | Time: 1.8m | ETA: 87.4m\n",
            "Step 1000/24415 | Loss: 0.3138 | LR: 0.000246 | Time: 3.6m | ETA: 85.2m\n",
            "Step 1500/24415 | Loss: 0.3891 | LR: 0.000300 | Time: 5.5m | ETA: 83.2m\n",
            "Step 2000/24415 | Loss: 0.5939 | LR: 0.000299 | Time: 7.3m | ETA: 81.4m\n",
            "Step 2500/24415 | Loss: 0.5058 | LR: 0.000298 | Time: 9.1m | ETA: 79.6m\n",
            "Step 3000/24415 | Loss: 0.6319 | LR: 0.000296 | Time: 10.9m | ETA: 77.8m\n",
            "Step 3500/24415 | Loss: 0.3762 | LR: 0.000293 | Time: 12.7m | ETA: 76.0m\n",
            "Step 4000/24415 | Loss: 0.2577 | LR: 0.000289 | Time: 14.5m | ETA: 74.2m\n",
            "Step 4500/24415 | Loss: 0.3761 | LR: 0.000285 | Time: 16.4m | ETA: 72.4m\n",
            "Step 5000/24415 | Loss: 0.3017 | LR: 0.000281 | Time: 18.2m | ETA: 70.5m\n",
            "Step 5500/24415 | Loss: 0.4483 | LR: 0.000275 | Time: 20.0m | ETA: 68.7m\n",
            "Step 6000/24415 | Loss: 0.5146 | LR: 0.000270 | Time: 21.8m | ETA: 66.9m\n",
            "Step 6500/24415 | Loss: 0.4340 | LR: 0.000263 | Time: 23.6m | ETA: 65.1m\n",
            "Step 7000/24415 | Loss: 0.4937 | LR: 0.000256 | Time: 25.4m | ETA: 63.3m\n",
            "Step 7500/24415 | Loss: 0.3025 | LR: 0.000249 | Time: 27.3m | ETA: 61.5m\n",
            "Step 8000/24415 | Loss: 0.5170 | LR: 0.000241 | Time: 29.1m | ETA: 59.7m\n",
            "Step 8500/24415 | Loss: 0.6257 | LR: 0.000233 | Time: 30.9m | ETA: 57.8m\n",
            "Step 9000/24415 | Loss: 0.4639 | LR: 0.000224 | Time: 32.7m | ETA: 56.0m\n",
            "Step 9500/24415 | Loss: 0.4980 | LR: 0.000215 | Time: 34.5m | ETA: 54.2m\n",
            "Step 10000/24415 | Loss: 0.3200 | LR: 0.000206 | Time: 36.4m | ETA: 52.4m\n",
            "Step 10500/24415 | Loss: 0.3511 | LR: 0.000196 | Time: 38.2m | ETA: 50.6m\n",
            "Step 11000/24415 | Loss: 0.5283 | LR: 0.000187 | Time: 40.0m | ETA: 48.8m\n",
            "Step 11500/24415 | Loss: 0.4333 | LR: 0.000177 | Time: 41.8m | ETA: 46.9m\n",
            "Step 12000/24415 | Loss: 0.3684 | LR: 0.000167 | Time: 43.6m | ETA: 45.1m\n",
            "Step 12500/24415 | Loss: 0.3064 | LR: 0.000156 | Time: 45.4m | ETA: 43.3m\n",
            "Step 13000/24415 | Loss: 0.3701 | LR: 0.000146 | Time: 47.3m | ETA: 41.5m\n",
            "Step 13500/24415 | Loss: 0.3867 | LR: 0.000136 | Time: 49.1m | ETA: 39.7m\n",
            "Step 14000/24415 | Loss: 0.3669 | LR: 0.000126 | Time: 50.9m | ETA: 37.9m\n",
            "Step 14500/24415 | Loss: 0.2955 | LR: 0.000116 | Time: 52.7m | ETA: 36.0m\n",
            "Step 15000/24415 | Loss: 0.4146 | LR: 0.000106 | Time: 54.5m | ETA: 34.2m\n",
            "Step 15500/24415 | Loss: 0.3501 | LR: 0.000097 | Time: 56.4m | ETA: 32.4m\n",
            "Step 16000/24415 | Loss: 0.3120 | LR: 0.000087 | Time: 58.2m | ETA: 30.6m\n",
            "Step 16500/24415 | Loss: 0.4151 | LR: 0.000078 | Time: 60.0m | ETA: 28.8m\n",
            "Step 17000/24415 | Loss: 0.3303 | LR: 0.000069 | Time: 61.8m | ETA: 27.0m\n",
            "Step 17500/24415 | Loss: 0.1501 | LR: 0.000061 | Time: 63.6m | ETA: 25.1m\n",
            "Step 18000/24415 | Loss: 0.4369 | LR: 0.000053 | Time: 65.4m | ETA: 23.3m\n",
            "Step 18500/24415 | Loss: 0.5476 | LR: 0.000046 | Time: 67.3m | ETA: 21.5m\n",
            "Step 19000/24415 | Loss: 0.4403 | LR: 0.000039 | Time: 69.1m | ETA: 19.7m\n",
            "Step 19500/24415 | Loss: 0.4612 | LR: 0.000032 | Time: 70.9m | ETA: 17.9m\n",
            "Step 20000/24415 | Loss: 0.3087 | LR: 0.000026 | Time: 72.7m | ETA: 16.0m\n",
            "Step 20500/24415 | Loss: 0.4692 | LR: 0.000021 | Time: 74.5m | ETA: 14.2m\n",
            "Step 21000/24415 | Loss: 0.4646 | LR: 0.000016 | Time: 76.3m | ETA: 12.4m\n",
            "Step 21500/24415 | Loss: 0.2407 | LR: 0.000012 | Time: 78.2m | ETA: 10.6m\n",
            "Step 22000/24415 | Loss: 0.5058 | LR: 0.000008 | Time: 80.0m | ETA: 8.8m\n",
            "Step 22500/24415 | Loss: 0.3871 | LR: 0.000005 | Time: 81.8m | ETA: 7.0m\n",
            "Step 23000/24415 | Loss: 0.5090 | LR: 0.000003 | Time: 83.6m | ETA: 5.1m\n",
            "Step 23500/24415 | Loss: 0.2955 | LR: 0.000001 | Time: 85.4m | ETA: 3.3m\n",
            "Step 24000/24415 | Loss: 0.3403 | LR: 0.000000 | Time: 87.2m | ETA: 1.5m\n",
            "\n",
            "Training completed in 88.8 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "val_loss_total = 0\n",
        "val_steps = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "        val_loss_total += loss.item()\n",
        "        val_steps += 1\n",
        "\n",
        "final_val_loss = val_loss_total / val_steps\n",
        "final_train_loss = sum(train_losses[-100:]) / min(100, len(train_losses))\n",
        "\n",
        "print(f\"Final train loss: {final_train_loss:.4f}\")\n",
        "print(f\"Final val loss: {final_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "iP73Y0ZVFu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7dbd695-fc1d-4142-ce7a-73e31a293a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final train loss: 0.3733\n",
            "Final val loss: 0.3487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), output_dir / 'best_model.pt')\n",
        "print(\"Best model saved\")"
      ],
      "metadata": {
        "id": "vdBVPTG3Fu23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6b6190-8f02-407a-f0af-0e78efb8bce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Set Perplexity"
      ],
      "metadata": {
        "id": "W2GVc3wHFu23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(456)\n",
        "test_max_idx = len(test_data) - CONTEXT_LENGTH - 1\n",
        "test_indices = np.random.choice(test_max_idx, size=10000, replace=False)\n",
        "\n",
        "test_dataset = MusicDataset(test_data, CONTEXT_LENGTH, test_indices)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "test_loss_total = 0\n",
        "test_steps = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "        test_loss_total += loss.item()\n",
        "        test_steps += 1\n",
        "\n",
        "test_loss = test_loss_total / test_steps\n",
        "perplexity = math.exp(test_loss)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Perplexity: {perplexity:.2f}\")"
      ],
      "metadata": {
        "id": "EdbW6g5gFu23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d2162d-99a9-432a-c1ad-b87e012ba21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.3495\n",
            "Perplexity: 1.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation Functions"
      ],
      "metadata": {
        "id": "q6VLuD08Fu23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_unconditional(model, max_length=150, temperature=1.0):\n",
        "    start_tokens = [\n",
        "        token2idx['<BOS>'],\n",
        "        token2idx.get('X:', token2idx['<UNK>']),\n",
        "        token2idx.get('M:4/4', token2idx['<UNK>']),\n",
        "        token2idx.get('L:1/8', token2idx['<UNK>'])\n",
        "    ]\n",
        "\n",
        "    keys = ['K:C', 'K:G', 'K:D', 'K:A', 'K:E']\n",
        "    random_key = keys[np.random.randint(0, len(keys))]\n",
        "    start_tokens.append(token2idx.get(random_key, token2idx['<UNK>']))\n",
        "\n",
        "    return generate(model, start_tokens, max_length, temperature)"
      ],
      "metadata": {
        "id": "nKvc0Ky-iQCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, start_tokens, max_length=200, temperature=1.0):\n",
        "    model.eval()\n",
        "    tokens = start_tokens.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            x = torch.tensor([tokens[-256:]]).to(device)\n",
        "            logits = model(x)\n",
        "            logits = logits[0, -1, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, 1).item()\n",
        "            tokens.append(next_token)\n",
        "            if next_token == token2idx.get('<EOS>', -1):\n",
        "                break\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "HqQI8qpfFu23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_text(tokens):\n",
        "    return ''.join([idx2token.get(t, '') for t in tokens])"
      ],
      "metadata": {
        "id": "HcLOkRH5Fu23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_abc_output(generated_text):\n",
        "    text = generated_text.replace('<BOS>', '').replace('<EOS>', '').replace('<PAD>', '')\n",
        "    text = text.replace('X:', 'X:1\\n')\n",
        "    text = text.replace('M:', 'M:')\n",
        "    text = text.replace('L:', '\\nL:')\n",
        "    text = text.replace('K:C', '\\nK:C\\n')\n",
        "    text = text.replace('K:G', '\\nK:G\\n')\n",
        "    text = text.replace('K:D', '\\nK:D\\n')\n",
        "    text = text.replace('K:A', '\\nK:A\\n')\n",
        "    text = text.replace('K:E', '\\nK:E\\n')\n",
        "    text = text.replace('K:F', '\\nK:F\\n')\n",
        "    text = text.replace('K:B', '\\nK:B\\n')\n",
        "\n",
        "    last_bar = text.rfind('|')\n",
        "    if last_bar > 0:\n",
        "        text = text[:last_bar+1]\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "dgM2ukr8Fu23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Unconditional Samples\n",
        "\n",
        "I tested two approaches for unconditional generation. First, I provided only the beginning token and let the model generate everything on its own. This produced invalid outputs like repeated rests or notes without proper ABC headers. This happens because headers appear only once per song at the start, while notes appear hundreds of times throughout. So the model saw headers rarely compared to notes during training. Second, I provided only the header tokens with randomized time signatures and keys, then let the model freely generate the melody. This produced valid, playable music since the headers provide the structure while the melody remains free."
      ],
      "metadata": {
        "id": "d1Z7EQ05Fu23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unconditional 1: Pure Unconditional"
      ],
      "metadata": {
        "id": "TofuZAh3v45U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unconditional_samples1 = []\n",
        "\n",
        "for i in range(5):\n",
        "    start = [token2idx['<BOS>']]\n",
        "    generated = generate(model, start, max_length=150, temperature=1)\n",
        "    text = tokens_to_text(generated)\n",
        "    cleaned = clean_abc_output(text)\n",
        "    unconditional_samples1.append(cleaned)\n",
        "    print(f\"\\n=== Unconditional Sample {i+1} ===\")\n",
        "    print(cleaned)"
      ],
      "metadata": {
        "id": "FRUJoiy1Fu23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75055fd8-c592-4747-9034-f4698b408e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Unconditional Sample 1 ===\n",
            "-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|C,,-496-|\n",
            "\n",
            "=== Unconditional Sample 2 ===\n",
            "|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|\n",
            "\n",
            "=== Unconditional Sample 3 ===\n",
            "G,/2z/2D,2A,>A,G,/2z/2D,/2z/2|A,,/2z/2A,/2z/2A,/2z/2C/2z/2E>CA,/2z/2G,/2G,/2|E,/2z/2E,/2z/2E,/2z/2E,/2z/2A,,/2z/2A,/2z/2A,/2z/2=B,/2z/2|[A,-G,]2A,/2-A,/2z/2_D>C=B,<_A,A,A,|D,>A,G,/2z/2D,/2=A,/2G,/2z/2F,/2>D,/2F,/2E,/2z/2G,/2|G,/2z/2D,/2z/2D,/2z/2A,,/2z/2A,/2z/2G,/2z/2F,/2z/2G,/2z/2|G,/2z/2D,/2_A,/2G,F,E,/2z/2G,/2z/2F,<G,D,/2|G,/2z/2D,/2z/2A,/2z/2=B,/2z/2A,/2z/2G,/2z/2A,/2z/2B,4-B,/2|\n",
            "\n",
            "=== Unconditional Sample 4 ===\n",
            "[FD-A,-D,-][A-DA,D,-][A-D-A,D,-][A-DA,D,-][AD-A,D,-][FDA,D,-]|[E-A,-D,-][E-D-A,D,-][EDA,D,-][E-A,-D,-][EDA,D,][^FD-A,-D,-]2[FDA,D,-][F-D-A,-D,-]|\n",
            "\n",
            "=== Unconditional Sample 5 ===\n",
            "^A,,2|zA,,2<G,,2G,,^D,,A,,-|^A,,2zA,,2<B,,2B,,|^D,,3/2z3/2D,,/2z/2^C,,C,,2z|F,,2zF,,/2z/2A,,2^A,,/2z3/2|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|^C,4-C,z2z/2^C,/2|B,,8|z8|z8|z8|z8|z8|z8|z8|z8|^C,4-C,z2z/2^C,/2|B,,8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unconditional 2: Header-Prompted Generation"
      ],
      "metadata": {
        "id": "aYxTq1Rgv7kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unconditional_samples2 = []\n",
        "\n",
        "for i in range(5):\n",
        "    generated = generate_unconditional(model, max_length=150, temperature=1)\n",
        "    text = tokens_to_text(generated)\n",
        "    cleaned = clean_abc_output(text)\n",
        "    unconditional_samples2.append(cleaned)\n",
        "    print(f\"\\n=== Unconditional Sample {i+1} ===\")\n",
        "    print(cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkFPOyb20vIo",
        "outputId": "9ee2dbd6-9767-4a5f-f1aa-0a09d72a5f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Unconditional Sample 1 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:G\n",
            "E,,4E,,3E,,/2E,,/2|C,,4C,,3C,,/2C,,/2|F,,,4F,,3-F,,/2F,,/2|E,,4E,,3E,,3/2E,,/2|C,,4C,,3-C,,/2C,,/2|F,,4F,,3-F,,/2F,,/2|E,,4E,,3E,,3/2E,,/2|C,,4C,,3C,,/2C,,/2|F,,4F,,3-F,,/2F,,/2|E,,4E,,3E,,2|C,,4C,,3C,,/2C,,/2|F,,4F,,3-F,,/2F,,/2|E,,4E,,3E,,3/2E,,/2|C,,4C,,3C,,/2C,,/2|F,,4F,,3-F,,/2F,,/2|E,,4E,,3E,,/2E,,/2|C,,3C,,3C,,/2C,,/2|F,,4F,,3-F,,/2F,,/2|E,,4E,,3[^CA,E,]2[CA,E,]2z/2[C-A,-E,-]2|\n",
            "\n",
            "=== Unconditional Sample 2 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:C\n",
            "z8|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|C,,C,,zA,,,B,,,2zB,,,|C,,3C,,z4|\n",
            "\n",
            "=== Unconditional Sample 3 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:E\n",
            "z8|z4|z8|z3G,/2z/2G,/2z/2A,/2z/2B,3/2z/2|A,3/2z/2C/2z/2C[D=D]2[EC]2[=G-D-]|[=G=D]3z4z|z8|z4[EC]2[FD]2|[BG]2[AF]2[GE]2[FD]G,[EC]B,[CB,]|[=D-B,-]3/2[DC-B,A,-]2[CA,]2[D-B,-]2[DB,G,]3/2DB,FB/2c/2-|c3/2z6z/2|C,6-C,-[C,B,,-]2B,,/2|\n",
            "\n",
            "=== Unconditional Sample 4 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:D\n",
            "[D-C-A,-^F,-D,,-D,,,-]8|[D-C-A,-^F,-D,,D,,,]3/2[dDCA,F,]2z[D-C-A,-F,-D,,-F,,,-][FDCA,F,D,,-F,,,-]2[FDCA,D,,-F,,,-]2[FDCA,D,,F,,,]2[DCA,F,]2[CA,F,D,,][DCA,F,D,,]2[D-C-A,-F,-D,,-]|\n",
            "\n",
            "=== Unconditional Sample 5 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:C\n",
            "z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z6zB-|BA2E2AG^F|A4-Az2A3/2A/2|^G^FE/2F/2ED4|z6zB/2B/2|BA/2z6z/2|z8|z3c/2c/2c/2c2BB|AG2z4z|z6AB|BAz6|z6zB/2B/2|BA/2z6z/2|z8|z3c/2c/2c/2c2BB|AG2z4z|z6AB|BA/2z6z/2|z8|z3c/2c/2c/2c2BB|AG4zB,G|F8|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Conditional Samples"
      ],
      "metadata": {
        "id": "29n9AkEpFu23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conditional_samples = []\n",
        "\n",
        "prefixes = [\n",
        "    ['<BOS>', 'X:', 'M:4/4', 'L:1/8', 'K:C', 'C', 'D', 'E', 'F'],\n",
        "    ['<BOS>', 'X:', 'M:4/4', 'L:1/8', 'K:G', 'G', 'A', 'B', 'c'],\n",
        "    ['<BOS>', 'X:', 'M:3/4', 'L:1/8', 'K:D', 'D', 'E', 'F'],\n",
        "    ['<BOS>', 'X:', 'M:4/4', 'L:1/8', 'K:A', 'A', 'B', 'c'],\n",
        "    ['<BOS>', 'X:', 'M:6/8', 'L:1/8', 'K:E', 'E', 'F', 'G']\n",
        "]\n",
        "\n",
        "for i, prefix in enumerate(prefixes):\n",
        "    prefix_tokens = [token2idx.get(t, token2idx['<UNK>']) for t in prefix]\n",
        "    generated = generate(model, prefix_tokens, max_length=150, temperature=1.0)\n",
        "    text = tokens_to_text(generated)\n",
        "    cleaned = clean_abc_output(text)\n",
        "    conditional_samples.append(cleaned)\n",
        "    print(f\"\\n=== Conditional Sample {i+1} ===\")\n",
        "    print(cleaned)"
      ],
      "metadata": {
        "id": "gj8XHkROFu23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685b8a74-6a16-4da5-f215-6cae78cff5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Conditional Sample 1 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:C\n",
            "CDEFG^GAF|FGECD3z|CD/2z/2GEFG^GA|cedfgabc'|z8|EG^Acdefd|cFGA/2z/2cz^dc|^Acd^dfgzd|G^GA/2z/2czdc/2z3/2|cdE/2z/2fe^A/2z/2dc|^Acd/2z/2^dc/2z/2c=d^d|gc^df=gf^g^a|z8|z8|z8|^Acd^dfg/2z/2dc|cAFGAcd^d|fd^AGz=A,,,z^A,,,|z^A/2z/2dc/2z/2czdc/2z/2|\n",
            "\n",
            "=== Conditional Sample 2 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:G\n",
            "GABcAGFG|A,,6-A,,3/2z/2|B=AGFED2C|B,4z2|GABcAGFG|E,6-E,3/2z/2|B=AGFED2C|B,4z2|GABcAGFG|A,,6-A,,3/2z/2|B=AGFED2C|CB,6-B,|GABcAGFG|A,,6-A,,3/2z/2|B=AGFED2C|B,C6-C|z8|z8|z8|z3/2CE/2-[G-E]2GG<GG/2-|G6CE|=G3FE3/2z/2CE-|\n",
            "\n",
            "=== Conditional Sample 3 ===\n",
            "X:1\n",
            "M:3/4\n",
            "L:1/8\n",
            "K:D\n",
            "DEFGA>fa-|a/2z3/2f'e'f'e'_g'|a'g'f'(3_e'f'g'f'_g'|a'g'f'(3=e'f'g'f'_d'|a'g'f'e'f'/2=e'd'|e'f'e'd'e'f'e'-|e'd'c'(3e'd'c'd'e'-|e'd'c'(3e'd'c'd''e'|f'_d'=d'e'f'g'f'|a'=g'f'e'e'3/2d'/2|e'f'e'd'e'_d'|e'f'e'd'e'f'e'|f'(3d'4c'4b4|[=cC]2[cC]3/2c/2[cC]3/2[BB,]2|\n",
            "\n",
            "=== Conditional Sample 4 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:A\n",
            "ABcBcdAB|\n",
            "\n",
            "=== Conditional Sample 5 ===\n",
            "X:1\n",
            "M:6/8\n",
            "L:1/8\n",
            "K:E\n",
            "EFGABcd|E4-EDE|F8-|F3EFGA|BAB8|BAcB2dcB|G4-GFGA|B8|=BBcBdcB=A|F,,8-|F,,3EF2GA|BAB8|BAcB4-B|BAcB2<d2cB|G4-GFGA|B8|=BBcBdcB=A|E8|z3EFGAB|A4-AFGA|B8|=BBcBdcB=A|E8-|E3EEFGB|c4-cefgf|d8|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate ABC Syntax"
      ],
      "metadata": {
        "id": "hjIO_dNGFu23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_abc(text):\n",
        "    has_x = 'X:' in text\n",
        "    has_m = 'M:' in text\n",
        "    has_k = 'K:' in text\n",
        "    has_notes = any(c in text for c in 'ABCDEFGabcdefg')\n",
        "    has_barlines = '|' in text\n",
        "    balanced_brackets = text.count('[') == text.count(']')\n",
        "    return has_x and has_m and has_k and has_notes and has_barlines and balanced_brackets\n",
        "\n",
        "all_samples = unconditional_samples1 + unconditional_samples2 + conditional_samples\n",
        "valid_count = 0\n",
        "valid_samples = []\n",
        "\n",
        "for s in all_samples:\n",
        "  if is_valid_abc(s):\n",
        "    valid_count += 1\n",
        "    valid_samples.append(s)\n",
        "\n",
        "print(f\"\\nValid ABC syntax: {valid_count}/{len(all_samples)} ({100*valid_count/len(all_samples):.1f}%)\")"
      ],
      "metadata": {
        "id": "ZNbYJjR8Fu23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c198cf1-c4b6-4d9f-8529-120588db4b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Valid ABC syntax: 10/15 (66.7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to MIDI"
      ],
      "metadata": {
        "id": "bxReUKnGFu24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install music21 -q"
      ],
      "metadata": {
        "id": "1gN6uuvzFu24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import converter, stream\n",
        "import copy\n",
        "\n",
        "def deep_clone_part(part):\n",
        "    new_part = stream.Part()\n",
        "    for el in part.recurse():\n",
        "        try:\n",
        "            new_part.insert(el.offset, copy.deepcopy(el))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return new_part\n",
        "\n",
        "midi_success = 0\n",
        "midi_total = 0\n",
        "\n",
        "for i, sample in enumerate(valid_samples):\n",
        "    midi_total += 1\n",
        "    try:\n",
        "        score = converter.parse(sample, format='abc')\n",
        "\n",
        "        fixed_score = stream.Score()\n",
        "\n",
        "        for part in score.parts if score.parts else [score]:\n",
        "            p = part\n",
        "\n",
        "            try:\n",
        "                if not p.getElementsByClass(stream.Measure):\n",
        "                    p = p.makeMeasures()\n",
        "                p = p.expandRepeats()\n",
        "                p = p.makeMeasures()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            p = deep_clone_part(p)\n",
        "\n",
        "            fixed_score.append(p)\n",
        "\n",
        "        midi_path = output_dir / f\"sample_{i+1}.mid\"\n",
        "        fixed_score.write(\"midi\", fp=str(midi_path))\n",
        "\n",
        "        abc_path = output_dir / f\"sample_{i+1}.abc\"\n",
        "        with open(abc_path, \"w\") as f:\n",
        "            f.write(sample)\n",
        "\n",
        "        midi_success += 1\n",
        "        print(f\"Sample {i+1}: MIDI conversion successful\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Sample {i+1}: MIDI conversion failed - {e}\")\n",
        "\n",
        "print(f\"\\nMIDI conversion success: {midi_success}/{midi_total} \"\n",
        "      f\"({100*midi_success/midi_total:.1f}%)\")"
      ],
      "metadata": {
        "id": "eANMUIdsFu24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad379f8-e5e5-4064-ca5b-97f013f21c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1: MIDI conversion successful\n",
            "Sample 2: MIDI conversion successful\n",
            "Sample 3: MIDI conversion successful\n",
            "Sample 4: MIDI conversion successful\n",
            "Sample 5: MIDI conversion successful\n",
            "Sample 6: MIDI conversion successful\n",
            "Sample 7: MIDI conversion successful\n",
            "Sample 8: MIDI conversion successful\n",
            "Sample 9: MIDI conversion successful\n",
            "Sample 10: MIDI conversion successful\n",
            "\n",
            "MIDI conversion success: 10/10 (100.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Summary"
      ],
      "metadata": {
        "id": "02Z2EmfnFu24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"PART 4 RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel: XL Transformer ({model.count_parameters():,} parameters)\")\n",
        "print(f\"Training: 100M additional tokens (200M total)\")\n",
        "print(f\"\\nFinal train loss: {final_train_loss:.4f}\")\n",
        "print(f\"Final val loss: {final_val_loss:.4f}\")\n",
        "print(f\"Test loss: {test_loss:.4f}\")\n",
        "print(f\"Test perplexity: {perplexity:.2f}\")\n",
        "print(f\"\\nGenerated samples: {len(all_samples)}\")\n",
        "print(f\"Valid ABC syntax: {valid_count}/{len(all_samples)} ({100*valid_count/len(all_samples):.1f}%)\")\n",
        "print(f\"MIDI conversion: {midi_success}/{midi_total} ({100*midi_success/midi_total:.1f}%)\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "-42BY9N9Fu24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534eb345-2645-45f0-881f-5295ca9f746c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PART 4 RESULTS SUMMARY\n",
            "============================================================\n",
            "\n",
            "Model: XL Transformer (156,788,736 parameters)\n",
            "Training: 100M additional tokens (200M total)\n",
            "\n",
            "Final train loss: 0.3733\n",
            "Final val loss: 0.3487\n",
            "Test loss: 0.3495\n",
            "Test perplexity: 1.42\n",
            "\n",
            "Generated samples: 15\n",
            "Valid ABC syntax: 10/15 (66.7%)\n",
            "MIDI conversion: 10/10 (100.0%)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Results"
      ],
      "metadata": {
        "id": "cLbHGHk6Fu24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    'model_params': model.count_parameters(),\n",
        "    'total_tokens_trained': 200_000_000,\n",
        "    'final_train_loss': final_train_loss,\n",
        "    'final_val_loss': final_val_loss,\n",
        "    'test_loss': test_loss,\n",
        "    'perplexity': perplexity,\n",
        "    'num_samples': len(all_samples),\n",
        "    'valid_abc_count': valid_count,\n",
        "    'valid_abc_percent': 100*valid_count/len(all_samples),\n",
        "    'midi_success_count': midi_success,\n",
        "    'midi_success_percent': 100*midi_success/midi_total,\n",
        "    'unconditional_samples1': unconditional_samples1,\n",
        "    'unconditional_samples2': unconditional_samples2,\n",
        "    'conditional_samples': conditional_samples\n",
        "}\n",
        "\n",
        "with open(output_dir / 'best_model_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Results saved to best_model_results.json\")"
      ],
      "metadata": {
        "id": "FQadgtcYFu24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da2b441-3a6e-4257-991e-76bd99aff651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to best_model_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1FHN4_ocgnKH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}