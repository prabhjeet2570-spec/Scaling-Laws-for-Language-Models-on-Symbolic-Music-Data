{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Data Collection and Preprocessing"
      ],
      "metadata": {
        "id": "N1iuZAvHkAoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y abcmidi"
      ],
      "metadata": {
        "id": "Lybi9fUgkAoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c5b8c6-5e88-4d23-8427-802288f4aa76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  abcm2ps timidity | pmidi postscript-viewer\n",
            "The following NEW packages will be installed:\n",
            "  abcmidi\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 306 kB of archives.\n",
            "After this operation, 868 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 abcmidi amd64 20220218+ds1-1 [306 kB]\n",
            "Fetched 306 kB in 0s (1,812 kB/s)\n",
            "Selecting previously unselected package abcmidi.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../abcmidi_20220218+ds1-1_amd64.deb ...\n",
            "Unpacking abcmidi (20220218+ds1-1) ...\n",
            "Setting up abcmidi (20220218+ds1-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import subprocess\n",
        "import json\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "p8FXFRaIkAoU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lwt4cCCRkAoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623b869a-9721-4be7-9bad-6a69e4561334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = Path('/content/drive/MyDrive/MLProject/data')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "kTzQ0GyxkAoV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract MIDI Files"
      ],
      "metadata": {
        "id": "UIoaf0A4kAoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/MyDrive/MLProject/data/lmd-dataset.zip'\n",
        "extract_path = '/content/lmd_dataset'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "midi_files = list(Path(extract_path).rglob('*.mid'))\n",
        "midi_files = [str(f) for f in midi_files]\n",
        "print(f\"Found {len(midi_files)} MIDI files\")"
      ],
      "metadata": {
        "id": "sp9dnJSXkAoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e517ad9-fb17-4685-a098-88084b1a851b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 116189 MIDI files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MIDI to ABC Conversion"
      ],
      "metadata": {
        "id": "9JmZpgimkAoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abc_output_dir = '/content/drive/MyDrive/MLProject/data/abc_files'\n",
        "os.makedirs(abc_output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "pOBoonupKCml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(args):\n",
        "    i, midi_path = args\n",
        "    out_fp = f'{abc_output_dir}/{i}.abc'\n",
        "\n",
        "    if os.path.exists(out_fp):\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        abc = subprocess.check_output(\n",
        "            ['midi2abc', midi_path],\n",
        "            stderr=subprocess.DEVNULL,\n",
        "            timeout=5\n",
        "        ).decode('utf-8')\n",
        "\n",
        "        with open(out_fp, 'w') as f:\n",
        "            f.write(abc)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "midi_files_indexed = list(enumerate(midi_files))\n",
        "\n",
        "with Pool(8) as p:\n",
        "    results = list(tqdm(p.imap(convert, midi_files_indexed), total=len(midi_files)))\n",
        "\n",
        "success_count = sum(results)\n",
        "fail_count = len(results) - success_count\n",
        "print(f\"\\nConversion complete: {success_count} succeeded, {fail_count} failed\")"
      ],
      "metadata": {
        "id": "djKwO1oAkAoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7e4d5e-dd19-46dc-bba3-0bbe97623ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 116189/116189 [11:20<00:00, 170.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Conversion complete: 115296 succeeded, 893 failed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc_files = list(Path(abc_output_dir).glob('*.abc'))[:50000]\n",
        "print(f\"Total ABC files: {len(abc_files)}\")"
      ],
      "metadata": {
        "id": "7bFvzcg8kAoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19115417-f802-4f99-9719-8a1ed222629d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total ABC files: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc_data = []\n",
        "for abc_file in tqdm(abc_files):\n",
        "    with open(abc_file, 'r', errors='ignore') as f:\n",
        "        content = f.read()\n",
        "    if content.strip():\n",
        "        abc_data.append(content)"
      ],
      "metadata": {
        "id": "9I5oSxAaqKV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7460e30-6f27-410c-8697-ef5f3f405113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [02:33<00:00, 325.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "1j2bXR2XkAoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_abc(abc_string):\n",
        "    tokens = []\n",
        "    lines = abc_string.strip().split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        if line.startswith('T:'):\n",
        "            continue\n",
        "        if line.startswith('%%'):\n",
        "            continue\n",
        "        if line.startswith('%'):\n",
        "            continue\n",
        "        if line.startswith('V:'):\n",
        "            continue\n",
        "        if line.startswith('Q:'):\n",
        "            continue\n",
        "\n",
        "        if len(line) > 1 and line[1] == ':':\n",
        "            field = line[0]\n",
        "            if field == 'X':\n",
        "                tokens.append('X:')\n",
        "            elif field == 'M':\n",
        "                value = line[2:].strip().split('%')[0].strip()\n",
        "                tokens.append(f'M:{value}')\n",
        "            elif field == 'L':\n",
        "                value = line[2:].strip().split('%')[0].strip()\n",
        "                tokens.append(f'L:{value}')\n",
        "            elif field == 'K':\n",
        "                value = line[2:].strip().split('%')[0].strip()\n",
        "                if value:\n",
        "                    tokens.append(f'K:{value[0]}')\n",
        "            continue\n",
        "\n",
        "        i = 0\n",
        "        while i < len(line):\n",
        "            if line[i] == '\\\\':\n",
        "                i += 1\n",
        "            elif line[i] == ' ':\n",
        "                i += 1\n",
        "            elif line[i] in '[]':\n",
        "                tokens.append(line[i])\n",
        "                i += 1\n",
        "            elif line[i] == '|':\n",
        "                tokens.append('|')\n",
        "                i += 1\n",
        "            elif line[i] in '^_=':\n",
        "                note = line[i]\n",
        "                i += 1\n",
        "                if i < len(line) and line[i] in 'ABCDEFGabcdefg':\n",
        "                    note += line[i]\n",
        "                    i += 1\n",
        "                    while i < len(line) and line[i] in \",'0123456789/\":\n",
        "                        note += line[i]\n",
        "                        i += 1\n",
        "                tokens.append(note)\n",
        "            elif line[i] in 'ABCDEFGabcdefg':\n",
        "                note = line[i]\n",
        "                i += 1\n",
        "                while i < len(line) and line[i] in \",'0123456789/\":\n",
        "                    note += line[i]\n",
        "                    i += 1\n",
        "                tokens.append(note)\n",
        "            elif line[i] == 'z':\n",
        "                rest = 'z'\n",
        "                i += 1\n",
        "                while i < len(line) and line[i] in '0123456789/':\n",
        "                    rest += line[i]\n",
        "                    i += 1\n",
        "                tokens.append(rest)\n",
        "            elif line[i] in '0123456789':\n",
        "                num = ''\n",
        "                while i < len(line) and line[i] in '0123456789/':\n",
        "                    num += line[i]\n",
        "                    i += 1\n",
        "                tokens.append(num)\n",
        "            elif line[i] in '-<>()':\n",
        "                tokens.append(line[i])\n",
        "                i += 1\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "FOJNYCRQPhr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_TOKENS = 30\n",
        "MAX_TOKENS = 5000\n",
        "\n",
        "all_token_sequences = []\n",
        "too_short_count = 0\n",
        "split_count = 0\n",
        "\n",
        "for abc in tqdm(abc_data):\n",
        "    tokens = tokenize_abc(abc)\n",
        "\n",
        "    if len(tokens) < MIN_TOKENS:\n",
        "        too_short_count += 1\n",
        "        continue\n",
        "\n",
        "    if len(tokens) <= MAX_TOKENS:\n",
        "        all_token_sequences.append(tokens)\n",
        "    else:\n",
        "        for j in range(0, len(tokens), MAX_TOKENS):\n",
        "            chunk = tokens[j:j + MAX_TOKENS]\n",
        "            if len(chunk) >= MIN_TOKENS:\n",
        "                all_token_sequences.append(chunk)\n",
        "                split_count += 1\n",
        "\n",
        "print(f\"Total sequences: {len(all_token_sequences)}\")\n",
        "print(f\"Filtered (too short): {too_short_count}\")\n",
        "print(f\"Split into chunks: {split_count}\")"
      ],
      "metadata": {
        "id": "uxm6l_B1dmmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db2156f8-bc8b-43cd-aba0-314fffd9a095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [11:56<00:00, 69.74it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequences: 263229\n",
            "Filtered (too short): 0\n",
            "Split into chunks: 257141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Vocabulary"
      ],
      "metadata": {
        "id": "_qdGEz2OkAoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_counts = Counter()\n",
        "for seq in all_token_sequences:\n",
        "    token_counts.update(seq)\n",
        "\n",
        "special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
        "sorted_tokens = sorted(token_counts.keys(), key=lambda x: -token_counts[x])\n",
        "\n",
        "token2idx = {}\n",
        "for i, tok in enumerate(special_tokens):\n",
        "    token2idx[tok] = i\n",
        "\n",
        "for tok in sorted_tokens:\n",
        "    if tok not in token2idx:\n",
        "        token2idx[tok] = len(token2idx)\n",
        "\n",
        "idx2token = {v: k for k, v in token2idx.items()}\n",
        "vocab_size = len(token2idx)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")"
      ],
      "metadata": {
        "id": "7EkLJnOwkAoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989352e8-0e0e-4604-be36-fe9566f01282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 27224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode Sequences"
      ],
      "metadata": {
        "id": "X_vmCwwQkAoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sequence(tokens, token2idx):\n",
        "    encoded = [token2idx['<BOS>']]\n",
        "    for tok in tokens:\n",
        "        if tok in token2idx:\n",
        "            encoded.append(token2idx[tok])\n",
        "        else:\n",
        "            encoded.append(token2idx['<UNK>'])\n",
        "    encoded.append(token2idx['<EOS>'])\n",
        "    return encoded\n",
        "\n",
        "encoded_sequences = []\n",
        "for seq in tqdm(all_token_sequences):\n",
        "    encoded_sequences.append(encode_sequence(seq, token2idx))"
      ],
      "metadata": {
        "id": "oyKpaS3AkAoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430aeaf2-5532-4394-9f13-398664042b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263229/263229 [01:47<00:00, 2451.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Val/Test Split"
      ],
      "metadata": {
        "id": "6sYjnvgikAoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "indices = np.random.permutation(len(encoded_sequences))\n",
        "shuffled_sequences = [encoded_sequences[i] for i in indices]\n",
        "\n",
        "n_total = len(shuffled_sequences)\n",
        "n_train = int(n_total * 0.98)\n",
        "n_val = int(n_total * 0.01)\n",
        "\n",
        "train_sequences = shuffled_sequences[:n_train]\n",
        "val_sequences = shuffled_sequences[n_train:n_train + n_val]\n",
        "test_sequences = shuffled_sequences[n_train + n_val:]\n",
        "\n",
        "train_flat = [tok for seq in train_sequences for tok in seq]\n",
        "val_flat = [tok for seq in val_sequences for tok in seq]\n",
        "test_flat = [tok for seq in test_sequences for tok in seq]\n",
        "\n",
        "train_array = np.array(train_flat, dtype=np.uint16)\n",
        "val_array = np.array(val_flat, dtype=np.uint16)\n",
        "test_array = np.array(test_flat, dtype=np.uint16)\n",
        "\n",
        "print(f\"Train: {len(train_sequences)} sequences, {len(train_array)} tokens\")\n",
        "print(f\"Val: {len(val_sequences)} sequences, {len(val_array)} tokens\")\n",
        "print(f\"Test: {len(test_sequences)} sequences, {len(test_array)} tokens\")"
      ],
      "metadata": {
        "id": "PST27UKaYGmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650bff21-fe7f-460b-e74d-f4d3f6802c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 257964 sequences, 1167894118 tokens\n",
            "Val: 2632 sequences, 11907471 tokens\n",
            "Test: 2633 sequences, 11808149 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Statistics"
      ],
      "metadata": {
        "id": "koK8QyKKkAoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_lengths = [len(seq) for seq in all_token_sequences]\n",
        "\n",
        "statistics = {\n",
        "    \"vocabulary_size\": vocab_size,\n",
        "    \"total_tokens\": {\n",
        "        \"train\": int(len(train_array)),\n",
        "        \"val\": int(len(val_array)),\n",
        "        \"test\": int(len(test_array)),\n",
        "        \"total\": int(len(train_array) + len(val_array) + len(test_array))\n",
        "    },\n",
        "    \"sequence_count\": {\n",
        "        \"train\": len(train_sequences),\n",
        "        \"val\": len(val_sequences),\n",
        "        \"test\": len(test_sequences),\n",
        "        \"total\": len(all_token_sequences)\n",
        "    },\n",
        "    \"sequence_length_distribution\": {\n",
        "        \"min\": int(np.min(sequence_lengths)),\n",
        "        \"max\": int(np.max(sequence_lengths)),\n",
        "        \"mean\": float(np.mean(sequence_lengths)),\n",
        "        \"median\": float(np.median(sequence_lengths)),\n",
        "        \"std\": float(np.std(sequence_lengths))\n",
        "    },\n",
        "    \"conversion_success_rate\": {\n",
        "        \"total_midi_files\": len(midi_files),\n",
        "        \"successful\": success_count,\n",
        "        \"failed\": fail_count,\n",
        "        \"success_rate\": round(success_count / len(midi_files) * 100, 2)\n",
        "    },\n",
        "    \"quality_filters\": {\n",
        "        \"min_tokens\": MIN_TOKENS,\n",
        "        \"max_tokens\": MAX_TOKENS,\n",
        "        \"filtered_too_short\": too_short_count,\n",
        "        \"sequences_from_splits\": split_count\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "SSRTJyGVkAoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"DATASET STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nVocabulary Size: {statistics['vocabulary_size']}\")\n",
        "print(f\"\\nTotal Tokens:\")\n",
        "print(f\"  Train: {statistics['total_tokens']['train']:,}\")\n",
        "print(f\"  Val: {statistics['total_tokens']['val']:,}\")\n",
        "print(f\"  Test: {statistics['total_tokens']['test']:,}\")\n",
        "print(f\"  Total: {statistics['total_tokens']['total']:,}\")\n",
        "print(f\"\\nSequence Count:\")\n",
        "print(f\"  Train: {statistics['sequence_count']['train']:,}\")\n",
        "print(f\"  Val: {statistics['sequence_count']['val']:,}\")\n",
        "print(f\"  Test: {statistics['sequence_count']['test']:,}\")\n",
        "print(f\"\\nSequence Length Distribution:\")\n",
        "print(f\"  Min: {statistics['sequence_length_distribution']['min']}\")\n",
        "print(f\"  Max: {statistics['sequence_length_distribution']['max']}\")\n",
        "print(f\"  Mean: {statistics['sequence_length_distribution']['mean']:.2f}\")\n",
        "print(f\"  Median: {statistics['sequence_length_distribution']['median']:.2f}\")\n",
        "print(f\"  Std: {statistics['sequence_length_distribution']['std']:.2f}\")\n",
        "print(f\"\\nConversion Success Rate:\")\n",
        "print(f\"  Total MIDI Files: {statistics['conversion_success_rate']['total_midi_files']:,}\")\n",
        "print(f\"  Successful: {statistics['conversion_success_rate']['successful']:,}\")\n",
        "print(f\"  Failed: {statistics['conversion_success_rate']['failed']:,}\")\n",
        "print(f\"  Success Rate: {statistics['conversion_success_rate']['success_rate']}%\")\n",
        "print(f\"\\nQuality Filters Applied:\")\n",
        "print(f\"  Min Tokens: {statistics['quality_filters']['min_tokens']}\")\n",
        "print(f\"  Max Tokens: {statistics['quality_filters']['max_tokens']}\")\n",
        "print(f\"  Filtered (too short): {statistics['quality_filters']['filtered_too_short']:,}\")\n",
        "print(f\"  Sequences from splits: {statistics['quality_filters']['sequences_from_splits']:,}\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "xSvXwI2MkAoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7dce6bd-7f08-4529-ef2f-19b6f16310bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DATASET STATISTICS\n",
            "============================================================\n",
            "\n",
            "Vocabulary Size: 27224\n",
            "\n",
            "Total Tokens:\n",
            "  Train: 1,167,894,118\n",
            "  Val: 11,907,471\n",
            "  Test: 11,808,149\n",
            "  Total: 1,191,609,738\n",
            "\n",
            "Sequence Count:\n",
            "  Train: 257,964\n",
            "  Val: 2,632\n",
            "  Test: 2,633\n",
            "\n",
            "Sequence Length Distribution:\n",
            "  Min: 30\n",
            "  Max: 5000\n",
            "  Mean: 4524.89\n",
            "  Median: 5000.00\n",
            "  Std: 1163.53\n",
            "\n",
            "Conversion Success Rate:\n",
            "  Total MIDI Files: 116,189\n",
            "  Successful: 115,296\n",
            "  Failed: 893\n",
            "  Success Rate: 99.23%\n",
            "\n",
            "Quality Filters Applied:\n",
            "  Min Tokens: 30\n",
            "  Max Tokens: 5000\n",
            "  Filtered (too short): 0\n",
            "  Sequences from splits: 257,141\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Files to Google Drive"
      ],
      "metadata": {
        "id": "YRtvkvwQkAoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(output_dir / 'train.npy', train_array)\n",
        "np.save(output_dir / 'val.npy', val_array)\n",
        "np.save(output_dir / 'test.npy', test_array)\n",
        "\n",
        "with open(output_dir / 'tokenizer.json', 'w') as f:\n",
        "    json.dump(token2idx, f)\n",
        "\n",
        "with open(output_dir / 'statistics.json', 'w') as f:\n",
        "    json.dump(statistics, f, indent=2)\n",
        "\n",
        "print(f\"Files saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "GIww1k9ZkAoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fedcf534-50d1-45ba-eff7-0d74212557e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files saved to /content/drive/MyDrive/MLProject/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file in output_dir.iterdir():\n",
        "    if not file.is_dir():\n",
        "        size = file.stat().st_size / 1024\n",
        "        print(f\"  {file.name}: {size:.1f} KB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib1B1r3DqMmq",
        "outputId": "1b2395b7-83e6-444b-a081-2f9fe586708c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lmd-dataset.zip: 1417133.0 KB\n",
            "train.npy: 2281043.3 KB\n",
            "val.npy: 23256.9 KB\n",
            "test.npy: 23062.9 KB\n",
            "statistics.json: 0.7 KB\n",
            "tokenizer.json: 412.5 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ySLkwkrorqmN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}