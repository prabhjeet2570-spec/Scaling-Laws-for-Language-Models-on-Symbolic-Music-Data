{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Best Model Training and Sample Generation"
      ],
      "metadata": {
        "id": "07-0wskBFu2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "CcK0DYEMFu2z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kZl0qZwjFu20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b7939b-ac75-411e-f4f0-555a617e8c34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = Path('/content/drive/MyDrive/MLProject/data')\n",
        "output_dir = Path('/content/drive/MyDrive/MLProject/results')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "AKUiFbs9Fu20"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "rc9NByxVFu20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af159c0-3a8c-4453-8733-9285acf5617e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "Memory: 85.2 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "r71FVZ3WFu20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load(data_dir / 'train.npy')\n",
        "val_data = np.load(data_dir / 'val.npy')\n",
        "test_data = np.load(data_dir / 'test.npy')\n",
        "\n",
        "with open(data_dir / 'tokenizer.json', 'r') as f:\n",
        "    token2idx = json.load(f)\n",
        "\n",
        "idx2token = {v: k for k, v in token2idx.items()}\n",
        "vocab_size = len(token2idx)\n",
        "\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "print(f\"Train tokens: {len(train_data):,}\")\n",
        "print(f\"Val tokens: {len(val_data):,}\")\n",
        "print(f\"Test tokens: {len(test_data):,}\")"
      ],
      "metadata": {
        "id": "bC2RH98BFu21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfabd81-4c76-4012-8191-d24080e78c5f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 27224\n",
            "Train tokens: 1,167,894,118\n",
            "Val tokens: 11,907,471\n",
            "Test tokens: 11,808,149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Part 4 Indices"
      ],
      "metadata": {
        "id": "sogB-IpKFu21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_LENGTH = 256\n",
        "NEW_NUM_SAMPLES = 100_000_000 // CONTEXT_LENGTH\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "train_max_idx = len(train_data) - CONTEXT_LENGTH - 1\n",
        "val_max_idx = len(val_data) - CONTEXT_LENGTH - 1\n",
        "\n",
        "train_indices_part4 = np.random.choice(train_max_idx, size=NEW_NUM_SAMPLES, replace=False)\n",
        "val_indices = np.load(data_dir / 'val_indices.npy')\n",
        "\n",
        "np.save(data_dir / 'train_indices_part4.npy', train_indices_part4)\n",
        "\n",
        "print(f\"Part 4 train indices: {len(train_indices_part4):,}\")\n",
        "print(f\"Val indices: {len(val_indices):,}\")"
      ],
      "metadata": {
        "id": "-9oIi8zBFu21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b107e78-c22e-4f55-9aef-d3a2d5f8ece7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 4 train indices: 390,625\n",
            "Val indices: 39,062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "krbIMYsLFu21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset(Dataset):\n",
        "    def __init__(self, data, context_length, indices):\n",
        "        self.data = torch.from_numpy(data.astype(np.int64))\n",
        "        self.context_length = context_length\n",
        "        self.indices = indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = self.indices[idx]\n",
        "        x = self.data[start:start + self.context_length]\n",
        "        y = self.data[start + 1:start + self.context_length + 1]\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "QqLrCR5GFu21"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Model"
      ],
      "metadata": {
        "id": "Vz9_gPp7Fu21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return self.out_proj(out)"
      ],
      "metadata": {
        "id": "p1C7kL6JFu22"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(F.gelu(self.fc1(x))))"
      ],
      "metadata": {
        "id": "TQOBWiFaFu22"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.dropout(self.attn(self.ln1(x), mask))\n",
        "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "X6uMPfhdFu22"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, context_length, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.context_length = context_length\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(context_length, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        d_ff = 4 * d_model\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        self.register_buffer('mask', torch.tril(torch.ones(context_length, context_length)).unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
        "        x = self.dropout(self.token_emb(x) + self.pos_emb(positions))\n",
        "\n",
        "        mask = self.mask[:, :, :seq_len, :seq_len]\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "lBOLx8ShFu22"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Configuration"
      ],
      "metadata": {
        "id": "TJb-zWQXFu22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_LENGTH = 256\n",
        "BATCH_TOKENS = 4096\n",
        "BATCH_SIZE = BATCH_TOKENS // CONTEXT_LENGTH\n",
        "LEARNING_RATE = 3e-4\n",
        "WARMUP_RATIO = 0.05\n",
        "\n",
        "MODEL_CONFIG = {'n_layers': 8, 'd_model': 1024, 'n_heads': 16}\n",
        "\n",
        "print(f\"Context length: {CONTEXT_LENGTH}\")\n",
        "print(f\"Batch size: {BATCH_SIZE} sequences ({BATCH_TOKENS} tokens)\")\n",
        "print(f\"Samples: {len(train_indices_part4):,}\")\n",
        "print(f\"Steps: {len(train_indices_part4) // BATCH_SIZE:,}\")"
      ],
      "metadata": {
        "id": "lKtKmamAFu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39868761-7ddf-492c-f7ac-403f777e4bca"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context length: 256\n",
            "Batch size: 16 sequences (4096 tokens)\n",
            "Samples: 390,625\n",
            "Steps: 24,414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pre-trained Model"
      ],
      "metadata": {
        "id": "BLJwHMgyFu22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=MODEL_CONFIG['d_model'],\n",
        "    n_heads=MODEL_CONFIG['n_heads'],\n",
        "    n_layers=MODEL_CONFIG['n_layers'],\n",
        "    context_length=CONTEXT_LENGTH\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(output_dir / 'xl_model.pt'))\n",
        "print(f\"Model loaded\")\n",
        "print(f\"Parameters: {model.count_parameters():,}\")"
      ],
      "metadata": {
        "id": "pdOZ6BcUFu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e93c747-d590-41b1-dc1a-28b7c6b7b540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded\n",
            "Parameters: 156,788,736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resume Training"
      ],
      "metadata": {
        "id": "1zAdrc6iFu22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(step, total_steps, warmup_steps, max_lr):\n",
        "    if step < warmup_steps:\n",
        "        return max_lr * step / warmup_steps\n",
        "    else:\n",
        "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
        "        return max_lr * 0.5 * (1 + math.cos(math.pi * progress))"
      ],
      "metadata": {
        "id": "1AmzsS20Fu22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset(train_data, CONTEXT_LENGTH, train_indices_part4)\n",
        "val_dataset = MusicDataset(val_data, CONTEXT_LENGTH, val_indices)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.1)\n",
        "\n",
        "total_steps = len(train_loader)\n",
        "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "\n",
        "print(f\"Total steps: {total_steps:,}\")\n",
        "print(f\"Warmup steps: {warmup_steps:,}\")"
      ],
      "metadata": {
        "id": "XmIk89P9Fu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a32d84a-b4b4-4bfa-cc45-747a5e8d96d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total steps: 24,415\n",
            "Warmup steps: 1,220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model.train()\n",
        "for step, (x, y) in enumerate(train_loader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    lr = get_lr(step, total_steps, warmup_steps, LEARNING_RATE)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    if step % 500 == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        remaining = (elapsed / (step + 1)) * (total_steps - step - 1)\n",
        "        print(f\"Step {step}/{total_steps} | Loss: {loss.item():.4f} | LR: {lr:.6f} | Time: {elapsed/60:.1f}m | ETA: {remaining/60:.1f}m\")\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {train_time/60:.1f} minutes\")"
      ],
      "metadata": {
        "id": "gdk7P5E8Fu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b85890cf-f88a-4663-fc54-3f1dd5a76338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0/24415 | Loss: 0.2398 | LR: 0.000000 | Time: 0.0m | ETA: 559.2m\n",
            "Step 500/24415 | Loss: 0.5309 | LR: 0.000123 | Time: 1.8m | ETA: 87.4m\n",
            "Step 1000/24415 | Loss: 0.3138 | LR: 0.000246 | Time: 3.6m | ETA: 85.2m\n",
            "Step 1500/24415 | Loss: 0.3891 | LR: 0.000300 | Time: 5.5m | ETA: 83.2m\n",
            "Step 2000/24415 | Loss: 0.5939 | LR: 0.000299 | Time: 7.3m | ETA: 81.4m\n",
            "Step 2500/24415 | Loss: 0.5058 | LR: 0.000298 | Time: 9.1m | ETA: 79.6m\n",
            "Step 3000/24415 | Loss: 0.6319 | LR: 0.000296 | Time: 10.9m | ETA: 77.8m\n",
            "Step 3500/24415 | Loss: 0.3762 | LR: 0.000293 | Time: 12.7m | ETA: 76.0m\n",
            "Step 4000/24415 | Loss: 0.2577 | LR: 0.000289 | Time: 14.5m | ETA: 74.2m\n",
            "Step 4500/24415 | Loss: 0.3761 | LR: 0.000285 | Time: 16.4m | ETA: 72.4m\n",
            "Step 5000/24415 | Loss: 0.3017 | LR: 0.000281 | Time: 18.2m | ETA: 70.5m\n",
            "Step 5500/24415 | Loss: 0.4483 | LR: 0.000275 | Time: 20.0m | ETA: 68.7m\n",
            "Step 6000/24415 | Loss: 0.5146 | LR: 0.000270 | Time: 21.8m | ETA: 66.9m\n",
            "Step 6500/24415 | Loss: 0.4340 | LR: 0.000263 | Time: 23.6m | ETA: 65.1m\n",
            "Step 7000/24415 | Loss: 0.4937 | LR: 0.000256 | Time: 25.4m | ETA: 63.3m\n",
            "Step 7500/24415 | Loss: 0.3025 | LR: 0.000249 | Time: 27.3m | ETA: 61.5m\n",
            "Step 8000/24415 | Loss: 0.5170 | LR: 0.000241 | Time: 29.1m | ETA: 59.7m\n",
            "Step 8500/24415 | Loss: 0.6257 | LR: 0.000233 | Time: 30.9m | ETA: 57.8m\n",
            "Step 9000/24415 | Loss: 0.4639 | LR: 0.000224 | Time: 32.7m | ETA: 56.0m\n",
            "Step 9500/24415 | Loss: 0.4980 | LR: 0.000215 | Time: 34.5m | ETA: 54.2m\n",
            "Step 10000/24415 | Loss: 0.3200 | LR: 0.000206 | Time: 36.4m | ETA: 52.4m\n",
            "Step 10500/24415 | Loss: 0.3511 | LR: 0.000196 | Time: 38.2m | ETA: 50.6m\n",
            "Step 11000/24415 | Loss: 0.5283 | LR: 0.000187 | Time: 40.0m | ETA: 48.8m\n",
            "Step 11500/24415 | Loss: 0.4333 | LR: 0.000177 | Time: 41.8m | ETA: 46.9m\n",
            "Step 12000/24415 | Loss: 0.3684 | LR: 0.000167 | Time: 43.6m | ETA: 45.1m\n",
            "Step 12500/24415 | Loss: 0.3064 | LR: 0.000156 | Time: 45.4m | ETA: 43.3m\n",
            "Step 13000/24415 | Loss: 0.3701 | LR: 0.000146 | Time: 47.3m | ETA: 41.5m\n",
            "Step 13500/24415 | Loss: 0.3867 | LR: 0.000136 | Time: 49.1m | ETA: 39.7m\n",
            "Step 14000/24415 | Loss: 0.3669 | LR: 0.000126 | Time: 50.9m | ETA: 37.9m\n",
            "Step 14500/24415 | Loss: 0.2955 | LR: 0.000116 | Time: 52.7m | ETA: 36.0m\n",
            "Step 15000/24415 | Loss: 0.4146 | LR: 0.000106 | Time: 54.5m | ETA: 34.2m\n",
            "Step 15500/24415 | Loss: 0.3501 | LR: 0.000097 | Time: 56.4m | ETA: 32.4m\n",
            "Step 16000/24415 | Loss: 0.3120 | LR: 0.000087 | Time: 58.2m | ETA: 30.6m\n",
            "Step 16500/24415 | Loss: 0.4151 | LR: 0.000078 | Time: 60.0m | ETA: 28.8m\n",
            "Step 17000/24415 | Loss: 0.3303 | LR: 0.000069 | Time: 61.8m | ETA: 27.0m\n",
            "Step 17500/24415 | Loss: 0.1501 | LR: 0.000061 | Time: 63.6m | ETA: 25.1m\n",
            "Step 18000/24415 | Loss: 0.4369 | LR: 0.000053 | Time: 65.4m | ETA: 23.3m\n",
            "Step 18500/24415 | Loss: 0.5476 | LR: 0.000046 | Time: 67.3m | ETA: 21.5m\n",
            "Step 19000/24415 | Loss: 0.4403 | LR: 0.000039 | Time: 69.1m | ETA: 19.7m\n",
            "Step 19500/24415 | Loss: 0.4612 | LR: 0.000032 | Time: 70.9m | ETA: 17.9m\n",
            "Step 20000/24415 | Loss: 0.3087 | LR: 0.000026 | Time: 72.7m | ETA: 16.0m\n",
            "Step 20500/24415 | Loss: 0.4692 | LR: 0.000021 | Time: 74.5m | ETA: 14.2m\n",
            "Step 21000/24415 | Loss: 0.4646 | LR: 0.000016 | Time: 76.3m | ETA: 12.4m\n",
            "Step 21500/24415 | Loss: 0.2407 | LR: 0.000012 | Time: 78.2m | ETA: 10.6m\n",
            "Step 22000/24415 | Loss: 0.5058 | LR: 0.000008 | Time: 80.0m | ETA: 8.8m\n",
            "Step 22500/24415 | Loss: 0.3871 | LR: 0.000005 | Time: 81.8m | ETA: 7.0m\n",
            "Step 23000/24415 | Loss: 0.5090 | LR: 0.000003 | Time: 83.6m | ETA: 5.1m\n",
            "Step 23500/24415 | Loss: 0.2955 | LR: 0.000001 | Time: 85.4m | ETA: 3.3m\n",
            "Step 24000/24415 | Loss: 0.3403 | LR: 0.000000 | Time: 87.2m | ETA: 1.5m\n",
            "\n",
            "Training completed in 88.8 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "val_loss_total = 0\n",
        "val_steps = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "        val_loss_total += loss.item()\n",
        "        val_steps += 1\n",
        "\n",
        "final_val_loss = val_loss_total / val_steps\n",
        "final_train_loss = sum(train_losses[-100:]) / min(100, len(train_losses))\n",
        "\n",
        "print(f\"Final train loss: {final_train_loss:.4f}\")\n",
        "print(f\"Final val loss: {final_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "iP73Y0ZVFu22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7dbd695-fc1d-4142-ce7a-73e31a293a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final train loss: 0.3733\n",
            "Final val loss: 0.3487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), output_dir / 'best_model.pt')\n",
        "print(\"Best model saved\")"
      ],
      "metadata": {
        "id": "vdBVPTG3Fu23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6b6190-8f02-407a-f0af-0e78efb8bce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Set Perplexity"
      ],
      "metadata": {
        "id": "W2GVc3wHFu23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(456)\n",
        "test_max_idx = len(test_data) - CONTEXT_LENGTH - 1\n",
        "test_indices = np.random.choice(test_max_idx, size=10000, replace=False)\n",
        "\n",
        "test_dataset = MusicDataset(test_data, CONTEXT_LENGTH, test_indices)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "test_loss_total = 0\n",
        "test_steps = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "        test_loss_total += loss.item()\n",
        "        test_steps += 1\n",
        "\n",
        "test_loss = test_loss_total / test_steps\n",
        "perplexity = math.exp(test_loss)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Perplexity: {perplexity:.2f}\")"
      ],
      "metadata": {
        "id": "EdbW6g5gFu23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91fc72d7-c4b5-4274-fab1-33241a61b3f4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.3495\n",
            "Perplexity: 1.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation Functions"
      ],
      "metadata": {
        "id": "q6VLuD08Fu23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_unconditional(model, max_length=150, temperature=1.0):\n",
        "    start_tokens = [\n",
        "        token2idx['<BOS>'],\n",
        "        token2idx.get('X:', token2idx['<UNK>']),\n",
        "        token2idx.get('M:4/4', token2idx['<UNK>']),\n",
        "        token2idx.get('L:1/8', token2idx['<UNK>'])\n",
        "    ]\n",
        "\n",
        "    # keys = ['K:C', 'K:G', 'K:D', 'K:A', 'K:E']\n",
        "    # random_key = keys[np.random.randint(0, len(keys))]\n",
        "    # start_tokens.append(token2idx.get(random_key, token2idx['<UNK>']))\n",
        "\n",
        "    return generate(model, start_tokens, max_length, temperature)"
      ],
      "metadata": {
        "id": "nKvc0Ky-iQCc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, start_tokens, max_length=200, temperature=1.0):\n",
        "    model.eval()\n",
        "    tokens = start_tokens.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            x = torch.tensor([tokens[-256:]]).to(device)\n",
        "            logits = model(x)\n",
        "            logits = logits[0, -1, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, 1).item()\n",
        "            tokens.append(next_token)\n",
        "            if next_token == token2idx.get('<EOS>', -1):\n",
        "                break\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "HqQI8qpfFu23"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_text(tokens):\n",
        "    return ''.join([idx2token.get(t, '') for t in tokens])"
      ],
      "metadata": {
        "id": "HcLOkRH5Fu23"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_abc_output(generated_text):\n",
        "    text = generated_text.replace('<BOS>', '').replace('<EOS>', '').replace('<PAD>', '')\n",
        "    text = text.replace('X:', 'X:1\\n')\n",
        "    text = text.replace('M:', 'M:')\n",
        "    text = text.replace('L:', '\\nL:')\n",
        "    text = text.replace('K:C', '\\nK:C\\n')\n",
        "    text = text.replace('K:G', '\\nK:G\\n')\n",
        "    text = text.replace('K:D', '\\nK:D\\n')\n",
        "    text = text.replace('K:A', '\\nK:A\\n')\n",
        "    text = text.replace('K:E', '\\nK:E\\n')\n",
        "    text = text.replace('K:F', '\\nK:F\\n')\n",
        "    text = text.replace('K:B', '\\nK:B\\n')\n",
        "\n",
        "    last_bar = text.rfind('|')\n",
        "    if last_bar > 0:\n",
        "        text = text[:last_bar+1]\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "dgM2ukr8Fu23"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Unconditional Samples\n",
        "\n",
        "I tested two approaches for unconditional generation. First, I provided only the beginning token and let the model generate everything on its own. This produced invalid outputs like repeated rests or notes without proper ABC headers. This happens because headers appear only once per song at the start, while notes appear hundreds of times throughout. So the model saw headers rarely compared to notes during training. Second, I provided only the header tokens with randomized time signatures and keys, then let the model freely generate the melody. This produced valid, playable music since the headers provide the structure while the melody remains free."
      ],
      "metadata": {
        "id": "d1Z7EQ05Fu23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unconditional 1: Pure Unconditional"
      ],
      "metadata": {
        "id": "TofuZAh3v45U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unconditional_samples1 = []\n",
        "\n",
        "for i in range(10):\n",
        "    start = [token2idx['<BOS>']]\n",
        "    generated = generate(model, start, max_length=150, temperature=1)\n",
        "    text = tokens_to_text(generated)\n",
        "    cleaned = clean_abc_output(text)\n",
        "    unconditional_samples1.append(cleaned)\n",
        "    print(f\"\\n=== Unconditional Sample {i+1} ===\")\n",
        "    print(cleaned)"
      ],
      "metadata": {
        "id": "FRUJoiy1Fu23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648c87ab-b61f-4645-c36c-4454efcaff58"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Unconditional Sample 1 ===\n",
            "X:1\n",
            "M:2/2\n",
            "L:1/8\n",
            "K:C\n",
            "z6|z6|A,,,6|G,,,2-G,,,/2z/2G,,,3-G,,,/2z/2|A,,,/2z/2A,,,4-A,,,|D,,,3zD,,,3/2z/2|E,,,4-E,,,F,,,|G,,,3zG,,,3-|[A,,,-G,,,]2A,,,3/2G,,,2-G,,,/2z/2A,,,|E,,,6-|E,,,6-|E,,,6-|E,,,4-E,,,z|z3/2E,-[A,-E,]2A,3/2B,z|z6|z2z/2G,,2z/2D,>D,|M:4/4|D,/2-[D-D,-]2[D-A,-D,-]6[DA,D,]2|E,8|D,8-|D,4-D,z|E,,,6|D,,,6|^G,,,6|A,,,6|B,,,6|E,,,6|A,,,6|G,,,6|B,,,6|E,,,6|A,,,6|\n",
            "\n",
            "=== Unconditional Sample 2 ===\n",
            "E,,-][E,,-E,,][E,,-B,,,]E,,-[E,,-D,,]E,,-|[E,,-B,,,]E,,-[E,,-E,,][E,,-B,,,]E,,-[E,,-D,,]E,,-|[E,,-B,,,]E,,-[E,,-E,,][E,,-B,,,]E,,-[E,,-D,,]E,,-|[E,,-B,,,]E,,-[E,,-E,,][E,,-B,,,]E,,-[E,,-D,,]E,,-|[E,,-B,,,]E,,-[E,,-E,,][E,,-B,,,]E,,-[E,,-D,,]E,,-|\n",
            "\n",
            "=== Unconditional Sample 3 ===\n",
            "]2A/2|[^cAE]2[cAE][cAE]2[cAE][cAE]2[cAE][cAE]2[cAE][cAF]2[cAF]2|[BGD][BGD]2[BGD][BGD]2[BGD][BGD]2[BG][BGD]2[BGD][BGD]2[BGD]2|\n",
            "\n",
            "=== Unconditional Sample 4 ===\n",
            "z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|\n",
            "\n",
            "=== Unconditional Sample 5 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8=FFF-[FE]2F-[FE]2F-[FE]2F-[FE]2F-[FE]2F-[FE]2F/2-[FE]2F-[FE]2F-[FE]2F-[FE]|\n",
            "\n",
            "=== Unconditional Sample 6 ===\n",
            "|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|z-428|\n",
            "\n",
            "=== Unconditional Sample 7 ===\n",
            "496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|z-496|\n",
            "\n",
            "=== Unconditional Sample 8 ===\n",
            "D,-[A,D,]D/2D/2A,|E,/2G,/2G,/2B,/2D2zD2|z2D,2E,3-E,/2D/2-|[DD,]A,/2A,/2D/2D/2A,-[A,E,]2G,/2G,2-[G,E,]2A,/2|z2D,/2F,<A,D<D,F,/2A,/2|z2E,/2G,<B,C<C,E,<A,E,/2|F,>B,,D,<F,D,/2F,2A,/2G,-[G-G,]2|[GC,]2z2(3CC^D,(3F,A,CF,/2|(3E,G,G,(3G,B,G,,(3^F,,D,F,(3A,B,G,|A,,2^C,E,/2E,/2(3A,,B,,C,(3D,F,E,|\n",
            "\n",
            "=== Unconditional Sample 9 ===\n",
            "z4z|z2[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2|[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2|[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2[GG,]2z/2|\n",
            "\n",
            "=== Unconditional Sample 10 ===\n",
            "A-F-C-A,-]3[A-FCA,-]2[AA,]2|[F^D^A,]4[^G-DC-G,-G,]3[GCG,]2z/2|[G^DCG,]3[GDCG,]zG,2-[D-G,]2D/2|F-[FD-]2D/2-[F-D]2F/2-[FD-]2D/2-[GD-]3/2D3/2F|[FD^A,]2z/2A,/2z/2F,[C=A,]2z/2C/2z/2FCz|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unconditional 2: Header-Prompted Generation"
      ],
      "metadata": {
        "id": "aYxTq1Rgv7kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unconditional_samples2 = []\n",
        "\n",
        "for i in range(10, 20):\n",
        "    generated = generate_unconditional(model, max_length=150, temperature=1)\n",
        "    text = tokens_to_text(generated)\n",
        "    cleaned = clean_abc_output(text)\n",
        "    unconditional_samples2.append(cleaned)\n",
        "    print(f\"\\n=== Unconditional Sample {i+1} ===\")\n",
        "    print(cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkFPOyb20vIo",
        "outputId": "f898f587-c24f-45cf-955a-08188db956b3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Unconditional Sample 11 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:B\n",
            "G,,G,,G,,G,,G,,G,,G,,G,,|G,,G,,G,,G,,G,,G,,G,,G,,|(3D,2A,,2G,,2(3G,,2A,,2G,,2|D,,2A,,,6-|A,,,8-|A,,,4E,,4|z8|[B,E,]12|[B,E,-]2[B,E,]3/2[B,E,-]2[B,E,][B,E,-]2[B,E,][B,E,-]2[B,E,][B,E,]2[B,E,-][B,E,]2B,/2E|E-[ED-]2D3/2-[DC-]2C3D2-|D2-[DA,-]2A,8-|\n",
            "\n",
            "=== Unconditional Sample 12 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:C\n",
            "z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|E,,3/2z2E,,/2E,,3/2z2|E,,3/2z2E,,/2E,,3/2z2|E,,>E,,E,,>B,,,E,,>B,,,E,,>B,,,|E,,z4E,,>D,,E,,z/2|E,,>E,,E,,B,,,>E,,B,,,>E,,D,,>E,,,|E,,z/2E,,E,,B,,,>E,,B,,,>E,,|E,,>D,,E,,z/2E,,>B,,,E,,z/2B,,,z/2|E,,,z2E,,,z/2B,,,3/2z2|E,,>D,,E,,z/2A,,,-[A,,,E,,,]2z2|\n",
            "\n",
            "=== Unconditional Sample 13 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:C\n",
            "z8|z8|z8|z8|z8|z8|[^fF]8|[eE]8|[^cC]4[cC]2[^AA,]2|[^GG,]8|[BB,]8|[^cC]8|[d-D-]8|[dD]6z2|^C,4F,4|z8|z8|z8|z8|z8|[dD]8|[^cC]8|[cC]8|[eE]8|[eE]8|[eE]8|[eE]8|[eE]8|[eE]8|[eE]8|\n",
            "\n",
            "=== Unconditional Sample 14 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:C\n",
            "z8|z8|z8|z8|z4C/2z/2CD=F|G>GG>G(3G2G2A2|d8-|d/2z/2d6-d|G3/2z/2G2>=F2GF|D8-|DzG,/2>B,/2(3B,CDG/2zG/2z|B>Bc>dBz/2c3/2-|c/2d2e2B3/2G2-G/2|zE/2C/2D/2z/2E/2z/2G3/2z/2G2|=F4zB,2G,/2>B,/2|C/2z/2D>DEz6|zC/2C/2z/2A,<CD<EG/2z/2G/2-|G=F2E2D3-|D4z3G,/2>B,/2|C/2z/2D>DE>G=F2-F/2z/2|\n",
            "\n",
            "=== Unconditional Sample 15 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:C\n",
            "z8|[D-D,-][AD-DD,-]2[D-D-B,-D,-]2[BDDB,D,]|[A-C-A,,-][A-E-C-CA,,-]2[A-E-C-C-A,,-]2[A-E-C-C-A,-A,,]2[AECCA,]2[eA-][gA]2[ge]2[G-E-D-B,-G,-][dG-E-D-C-B,-G,-]2[cGEDCB,G,]2|\n",
            "\n",
            "=== Unconditional Sample 16 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:B\n",
            "z8|z8|F,/2zE,,,3/2z2C,,/2z/2F,,,3/2z/2F,,,|B,,,/2z/2B,,,zB,,,/2z/2B,,,/2z/2B,,,/2z/2B,,,/2z/2B,,,|B,,,/2z/2B,,,z3B,,,/2z/2B,,,/2z/2B,,,|B,,,/2z/2B,,,zB,,,/2z/2B,,,/2z/2B,,,/2z/2B,,,/2z/2B,,,|B,,,/2z/2B,,,z3B,,,/2z/2B,,,/2z/2B,,,|B,,,/2z/2B,,,zB,,,/2z/2B,,,/2z/2B,,,/2z/2B,,,/2z/2B,,,|B,,,/2z/2B,,,zB,,,/2z/2B,,,/2z/2B,,,/2z/2B,,,/2z/2|B,,,/2z/2B,,,z3B,,,/2z/2B,,,/2z/2B,,,|B,,,/2z/2B,,,/2z/2B,,,/2z2z/2B,,,/2z/2B,,,/2z/2B,,,|B,,,/2z/2B,,,zB,,,/2z2z/2B,,,/2z/2B,,,|B,,,/2z/2B,,,zB,,,/2z2z/2B,,,/2z/2B,,,|B,,,/2z/2B,,,zB,,,/2z2z/2B,,,/2z/2B,,,/2z/2|\n",
            "\n",
            "=== Unconditional Sample 17 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:C\n",
            "z8|[=F-D-]2[B-F-D-]3[BF-D-]2[FD]2|z8|z/2[E-C-]3[G-EC]4[G-D-]2|[G-D-]2[GE-D-]3[ED]2[DB,-]3[C-B,A,-]2|[C-A,-]3[CA,]2z4z/2|[d-B-]2[d-B-G-]2[d-B-G]2[dB]2z6z/2|z8|\n",
            "\n",
            "=== Unconditional Sample 18 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:C\n",
            "z8|z8|z8|z8|z8|z8|z8|z8|z6z2|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|z4z4|\n",
            "\n",
            "=== Unconditional Sample 19 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:C\n",
            "z8|z8|z2z2z2z2|z2z2z2z2|z2z2z2z2|z2z2z2z2|z2z2z2z2|z2z2z2z2|z2z2z2z3/2z/2|z2z2zzzz|z3/2z/2z2zzz3/2z/2|z3/2zz3/2B,,,/2z/2zB,,,/2z3/2z/2|z3/2zz3/2B,,,/2z/2zB,,,/2z3/2z/2|z3/2zz3/2B,,,/2zB,,,/2z2|z3/2zz3/2B,,,/2z/2zB,,,/2z3/2z/2|z3/2zz3/2B,,,/2z/2zzz|z3/2zz2zzz^a/2-|^a/2zz2z2z2zzz|z3/2zz2z2z2zz/2|z3/2z2z2z2zz/2|z3/2z2z2z2zz/2|z3/2z2z2z3/2zz/2|\n",
            "\n",
            "=== Unconditional Sample 20 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:B\n",
            "z8|z8|[A,=G,A,,A,,D,D,,-]2[A,G,A,,-D,,-]2[A,G,A,,-D,,-]2[A,G,A,,-D,,-]2[G,A,,-D,,-]2[A,G,A,,-D,,-]2[A,G,A,,-D,,-]2[A,G,A,,-D,,-]2[G,A,,-D,,]2[A,G,A,,]2|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Conditional Samples"
      ],
      "metadata": {
        "id": "29n9AkEpFu23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conditional_samples = []\n",
        "\n",
        "prefixes = [\n",
        "    ['<BOS>', 'X:', 'M:4/4', 'L:1/8', 'K:C', 'C', 'D', 'E', 'F'],\n",
        "    ['<BOS>', 'X:', 'M:4/4', 'L:1/8', 'K:G', 'G', 'A', 'B', 'c'],\n",
        "    ['<BOS>', 'X:', 'M:3/4', 'L:1/8', 'K:D', 'D', 'E', 'F'],\n",
        "    ['<BOS>', 'X:', 'M:4/4', 'L:1/8', 'K:A', 'A', 'B', 'c'],\n",
        "    ['<BOS>', 'X:', 'M:6/8', 'L:1/8', 'K:E', 'E', 'F', 'G'],\n",
        "    ['<BOS>', 'X:', 'M:4/4', 'L:1/8', 'K:F', 'F', 'G', 'A', 'B'],\n",
        "    ['<BOS>', 'X:', 'M:2/4', 'L:1/8', 'K:C', 'G,', 'C', 'E', 'G'],\n",
        "    ['<BOS>', 'X:', 'M:4/4', 'L:1/8', 'K:D', 'A,', 'D', 'F', 'A'],\n",
        "    ['<BOS>', 'X:', 'M:3/4', 'L:1/8', 'K:G', 'D', 'G', 'B', 'd'],\n",
        "    ['<BOS>', 'X:', 'M:6/8', 'L:1/8', 'K:A', 'E', 'A', 'c', 'e']\n",
        "]\n",
        "\n",
        "for i, prefix in enumerate(prefixes):\n",
        "    prefix_tokens = [token2idx.get(t, token2idx['<UNK>']) for t in prefix]\n",
        "    generated = generate(model, prefix_tokens, max_length=150, temperature=1.0)\n",
        "    text = tokens_to_text(generated)\n",
        "    cleaned = clean_abc_output(text)\n",
        "    conditional_samples.append(cleaned)\n",
        "    print(f\"\\n=== Conditional Sample {i+1} ===\")\n",
        "    print(cleaned)"
      ],
      "metadata": {
        "id": "gj8XHkROFu23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c949c254-3163-4cb4-8d32-5b068919f49e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Conditional Sample 1 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:C\n",
            "CDEFGAcB,|^A,B,DGGDCA,|CDEFGAcB,|C3/2z2z/2EFzG-|GF/2z/2F/2F/2AGCzD|EFEDzEFz|(3cccccccc|GG3/2z/2GGA/2z/2A/2z/2A/2A/2|c/2z/2c/2z/2cc/2z/2edc/2z/2^d/2c/2|G/2-[GE-]2E3/2zE/2E/2EE^D|C^A,3/2z/2G,/2G,/2G,F,E,/2F,/2G,/2A,/2|CCCC/2z/2C/2C/2CCz|E/2E/2E/2z/2C/2DCC3/2z3/2|\n",
            "\n",
            "=== Conditional Sample 2 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:G\n",
            "GABcdzB,E|FGFABz3|G/2F/2GA/2B/2GcdcB|GABcdzB,G|FGABcdzB,/2z/2|GABcdzB,E|FGFABzA,G|FGFABz3|G/2F/2GABcdzB,|GABcdzB,E|FGFABzA,G|FGFABzG,A|GABcdzB,E|FGFABzG,A|GABcdzB,E|FGFABzF,A|GABcdzB,E|\n",
            "\n",
            "=== Conditional Sample 3 ===\n",
            "X:1\n",
            "M:3/4\n",
            "L:1/8\n",
            "K:D\n",
            "DEFEF2FF-|FGFEGGAA-|AGc_A2z2|EFGFG2FF-|FE2FEFFG-|G_GFEEz2E|GAAAA2GG|GAGAA2GG|GFGFE2G_A-|_A2GGGFFG-|G_GFEDEEE|_GAA2AGGG|GF2GGGAA|AGAGGGFG-|G_G2F2G3|z8|z8|z8|M:5/4GA|GFGFE2GG-|G_A=AGAG2A|\n",
            "\n",
            "=== Conditional Sample 4 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:A\n",
            "ABce4-e3/2z/2|ABce3-e/2z3/2|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|zABce4-e/2z/2|ABc<e2z12|z12|z12|z12|z12|z12|z12|z3[AF]2[GE][^G-D-]2|[^GD]3/2z/2[FD][=G-D-]4[G-D-]2|[GD][GB,][GB,][A-C-]6[AC]|z8z2|\n",
            "\n",
            "=== Conditional Sample 5 ===\n",
            "X:1\n",
            "M:6/8\n",
            "L:1/8\n",
            "K:E\n",
            "EFGcBGA|=E8-|E/2z6z3/2|ABcedcdA|B8-|B/2z6z3/2|abc'd'c'd'c'b2|afe4-ec2|dcdbad'2z|abc'd'c'bac'|d'8-|d'2z2b2=g2|f8-|f4=E,,2c'2|=f6b2|a6e2|_e3e2a2|=gfefe2zB|c'8-|[c'-c]2c'6-c'3/2|d'8-|d'6a2|=abc'd'c'b2b|[c'c]6z2|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|\n",
            "\n",
            "=== Conditional Sample 6 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:F\n",
            "FGABcd/2A/2Gc|[a_g]f_dfgfdf|[bg]abagfefdefgf|G2_GBec'geg^g2|a2g_gfde/2d/2B2z4|z4G2e2|a2g3/2f/2e/2d/2c2zcB|cBGcABGED|C3B2c2dc|ddedcBcdB|c2BGcAGEB|ccBGBcAGF|A3Bcde2BcF,|]3/2z/2B3/2_B/2c3Bc2|F3|A2B6|z4G2d2|\n",
            "\n",
            "=== Conditional Sample 7 ===\n",
            "X:1\n",
            "M:2/4\n",
            "L:1/8\n",
            "K:C\n",
            "G,CEGGec^A,G,|CA,EGGacG,-|G,cGCA,CEG|cAEAc'ecG,|CA,EGcece|dF,,A,DFAdA,,|^DF^AdFaeD,-|[^DD,]F^AdFaeF|G,CEGGecG,|C,=CEGGecG,|C,=CEGGecG,|C,CEGGecG,|D,A,DFAdA,|D,A,DFAdfa|^A,,A,F=AAdfa|^A,,A,F=AAdfa|\n",
            "\n",
            "=== Conditional Sample 8 ===\n",
            "X:1\n",
            "M:4/4\n",
            "L:1/8\n",
            "K:D\n",
            "A,DFAz/2c2z/2c|z8|z8|z8|z8|A,DFA2cdf|A3/2z/2CDFGA2|FDDd2Acf|AFc2d4-|d2AFGFD2|G,CDA2ccf|AFD2G,2CD-|D2GDGDGD|GDGDA,G,F,G,|E,4z2A,D|FAcdAFG2|AcfA2F4-|F2FFAc2f|g2AG2^Adf|d4z2^A/2A/2A|c4z2^A/2A/2A|f2dczd/2d/2dd/2d/2|c2c^Ac=A/2c/2Ac|\n",
            "\n",
            "=== Conditional Sample 9 ===\n",
            "X:1\n",
            "M:3/4\n",
            "L:1/8\n",
            "K:G\n",
            "DGBdBG^c|GBdBGB^c|G^cgcGc^f|dAd^gdG^c|GBd4z|D3^F^cdf|d2edd^c|z8|d6-d/2z3/2|^CDADADB|^cAdADDB|^cA-[e-A]2e/2Bz^dGB|[d^F-]3[^c-F]c2-c/2z3/2|E^FAFAFB|^c8|z3ABD2z|dddd^fdfe|z8|d3^F^cdf|d3A^dAd|^cAdADDB|\n",
            "\n",
            "=== Conditional Sample 10 ===\n",
            "X:1\n",
            "M:6/8\n",
            "L:1/8\n",
            "K:A\n",
            "EAcec'ec'e|bafg'e'c'B|egabc'dF|Dfebeab|f2abc'dE|DfE,d'fed|B,edB,e'AF|Cec]eaecA|Eeabc'dD|[GC]8|[EB,]8|[FB,]3[FB,][AE]2z2z/2|[FA,]8|[GB,]8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|z8|[ECA,]8|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate ABC Syntax"
      ],
      "metadata": {
        "id": "hjIO_dNGFu23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_abc(text):\n",
        "    has_x = 'X:' in text\n",
        "    has_m = 'M:' in text\n",
        "    has_k = 'K:' in text\n",
        "    has_notes = any(c in text for c in 'ABCDEFGabcdefg')\n",
        "    has_barlines = '|' in text\n",
        "    balanced_brackets = text.count('[') == text.count(']')\n",
        "    return has_x and has_m and has_k and has_notes and has_barlines and balanced_brackets\n",
        "\n",
        "all_samples = unconditional_samples1 + unconditional_samples2 + conditional_samples\n",
        "valid_count = 0\n",
        "valid_samples = []\n",
        "\n",
        "for s in all_samples:\n",
        "  if is_valid_abc(s):\n",
        "    valid_count += 1\n",
        "    valid_samples.append(s)\n",
        "\n",
        "print(f\"\\nValid ABC syntax: {valid_count}/{len(all_samples)} ({100*valid_count/len(all_samples):.1f}%)\")"
      ],
      "metadata": {
        "id": "ZNbYJjR8Fu23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75824b23-642b-4d0e-e623-eb45b3e14462"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Valid ABC syntax: 19/30 (63.3%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to MIDI"
      ],
      "metadata": {
        "id": "bxReUKnGFu24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install music21 -q"
      ],
      "metadata": {
        "id": "1gN6uuvzFu24"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import converter, stream, meter\n",
        "\n",
        "midi_success = 0\n",
        "midi_total = 0\n",
        "\n",
        "for i, sample in enumerate(valid_samples):\n",
        "    midi_total += 1\n",
        "    try:\n",
        "        score = converter.parse(sample, format=\"abc\")\n",
        "        score.makeNotation(inPlace=True)\n",
        "        flat = score.flatten()\n",
        "        clean_part = stream.Part()\n",
        "        seen_ts = set()\n",
        "\n",
        "        for el in flat:\n",
        "            if isinstance(el, meter.TimeSignature):\n",
        "                ts_key = (el.numerator, el.denominator)\n",
        "                if ts_key in seen_ts:\n",
        "                    continue\n",
        "                seen_ts.add(ts_key)\n",
        "\n",
        "            clean_part.append(el)\n",
        "\n",
        "        clean_part.quantize([0.25, 0.5, 1.0], inPlace=True)\n",
        "\n",
        "        midi_path = output_dir / f\"sample_{i+1}.mid\"\n",
        "        clean_part.write(\"midi\", fp=str(midi_path))\n",
        "\n",
        "        abc_path = output_dir / f\"sample_{i+1}.abc\"\n",
        "        with open(abc_path, \"w\") as f:\n",
        "            f.write(sample)\n",
        "\n",
        "        midi_success += 1\n",
        "        print(f\"Sample {i+1}: MIDI conversion successful\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Sample {i+1}: MIDI conversion failed - {e}\")\n",
        "\n",
        "print(\n",
        "    f\"\\nMIDI conversion success: {midi_success}/{midi_total} \"\n",
        "    f\"({100 * midi_success / midi_total:.1f}%)\"\n",
        ")"
      ],
      "metadata": {
        "id": "eANMUIdsFu24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cff4ba0-9e3a-4403-ad53-42735f84374a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1: MIDI conversion failed - too many values to unpack (expected 2)\n",
            "Sample 2: MIDI conversion successful\n",
            "Sample 3: MIDI conversion successful\n",
            "Sample 4: MIDI conversion successful\n",
            "Sample 5: MIDI conversion successful\n",
            "Sample 6: MIDI conversion successful\n",
            "Sample 7: MIDI conversion successful\n",
            "Sample 8: MIDI conversion successful\n",
            "Sample 9: MIDI conversion successful\n",
            "Sample 10: MIDI conversion successful\n",
            "Sample 11: MIDI conversion successful\n",
            "Sample 12: MIDI conversion successful\n",
            "Sample 13: MIDI conversion successful\n",
            "Sample 14: MIDI conversion failed - Cannot set partition by 5 (5/422)\n",
            "Sample 15: MIDI conversion successful\n",
            "Sample 16: MIDI conversion successful\n",
            "Sample 17: MIDI conversion successful\n",
            "Sample 18: MIDI conversion successful\n",
            "Sample 19: MIDI conversion successful\n",
            "\n",
            "MIDI conversion success: 17/19 (89.5%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Summary"
      ],
      "metadata": {
        "id": "02Z2EmfnFu24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"PART 4 RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel: XL Transformer ({model.count_parameters():,} parameters)\")\n",
        "print(f\"Training: 100M additional tokens (200M total)\")\n",
        "print(f\"\\nFinal train loss: {final_train_loss:.4f}\")\n",
        "print(f\"Final val loss: {final_val_loss:.4f}\")\n",
        "print(f\"Test loss: {test_loss:.4f}\")\n",
        "print(f\"Test perplexity: {perplexity:.2f}\")\n",
        "print(f\"\\nGenerated samples: {len(all_samples)}\")\n",
        "print(f\"Valid ABC syntax: {valid_count}/{len(all_samples)} ({100*valid_count/len(all_samples):.1f}%)\")\n",
        "print(f\"MIDI conversion: {midi_success}/{midi_total} ({100*midi_success/midi_total:.1f}%)\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "-42BY9N9Fu24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3296810e-9e33-43b0-ff45-9459e0c3b11e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PART 4 RESULTS SUMMARY\n",
            "============================================================\n",
            "\n",
            "Model: XL Transformer (156,788,736 parameters)\n",
            "Training: 100M additional tokens (200M total)\n",
            "\n",
            "Final train loss: 0.3733\n",
            "Final val loss: 0.3480\n",
            "Test loss: 0.3495\n",
            "Test perplexity: 1.42\n",
            "\n",
            "Generated samples: 30\n",
            "Valid ABC syntax: 19/30 (63.3%)\n",
            "MIDI conversion: 17/19 (89.5%)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Results"
      ],
      "metadata": {
        "id": "cLbHGHk6Fu24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    'model_params': model.count_parameters(),\n",
        "    'total_tokens_trained': 200_000_000,\n",
        "    'final_train_loss': final_train_loss,\n",
        "    'final_val_loss': final_val_loss,\n",
        "    'test_loss': test_loss,\n",
        "    'perplexity': perplexity,\n",
        "    'num_samples': len(all_samples),\n",
        "    'valid_abc_count': valid_count,\n",
        "    'valid_abc_percent': 100*valid_count/len(all_samples),\n",
        "    'midi_success_count': midi_success,\n",
        "    'midi_success_percent': 100*midi_success/midi_total,\n",
        "    'unconditional_samples1': unconditional_samples1,\n",
        "    'unconditional_samples2': unconditional_samples2,\n",
        "    'conditional_samples': conditional_samples\n",
        "}\n",
        "\n",
        "with open(output_dir / 'best_model_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Results saved to best_model_results.json\")"
      ],
      "metadata": {
        "id": "FQadgtcYFu24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb447de-9f57-40b9-9fd1-6d3759ec6237"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to best_model_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLX0_AWV6euf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}